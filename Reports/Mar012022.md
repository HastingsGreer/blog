@def title = "March 1"


# Atlas registration progress

At low resolution

Since last time, found that I could not train ICON_atlas loss to match performance of ICON on OAI knees:
second step of multiscale training failed as folds ran away, lambda increased to infinity.

# Outreach:

Presenting same powerpoint from group meeting last week to funky bunch on wednesday. Several people from 
Kitware's AI team coming: anything I should make sure to include?


# Follow up to "New approach to patchwise registration"
Wanted to setup for applying [Instant Neural Graphics Primitives](https://nvlabs.github.io/instant-ngp/) to neural registration fields. 
To do this, I needed a valid regularization proceedure. Started with this approach:

\fig{nedf.drawio.png}

Which then doesn't work: produces very irregular mappings or identity map:

[colab notebook](https://colab.research.google.com/github/HastingsGreer/InverseConsistency/blob/master/notebooks/NEDF.ipynb)

New approach: Use torch.autograd to compute spatial gradients of node "Approximation of original position in Image A, - Position in Image A" with respect to node "Position in Image A"

Square these partial derivatives and minimize the result. This is inspired by the gradient penalty for GANS espoused in [When do GANS actually converge](https://arxiv.org/abs/1801.04406) which we discussed a while back: Each sample forces a neighborhood around it to be near zero, instead of just a single point.

FOR A SINGLE PAIR, THIS WORKS FOR REGISTRATION!

[notebook](https://colab.research.google.com/drive/1aCzG7tUwDjnlGvcUBAdSIkpVKWtuDq76?usp=sharing)

Image A

\fig{nedf_A}

Image B

\fig{nedf_B}

Grid

\fig{grid}

Warped B

\fig{nedf_warped_B}


# GradientICON

While getting the above to work, I was impressed enough with the performance of the Jacobian penalty on the Inverse Consistency term to try it back on the standard convolutional ICON.

I computed the jacobian using finite differences instead of torch.autograd since that was more convenient, and it's only through linear interpolations, so finite differences are usually exact anyways.

```python

delta = .001

if len(self.identityMap.shape) == 4:
    dx = torch.Tensor([[[[delta]], [[0.]]]]).to(config.device)
    dy = torch.Tensor([[[[0.]], [[delta]]]]).to(config.device)
    direction_vectors = (dx, dy)

elif len(self.identityMap.shape) == 5:
    dx = torch.Tensor([[[[[delta]]], [[[0.]]], [[[0.]]]]]).to(config.device)
    dy = torch.Tensor([[[[[0.]]], [[[delta]]], [[[0.]]]]]).to(config.device)
    dz = torch.Tensor([[[[0.]]], [[[0.]]], [[[delta]]]]).to(config.device)
    direction_vectors = (dx, dy, dz)

for d in direction_vectors:
    approximate_Iepsilon_d = self.phi_AB(self.phi_BA(Iepsilon + d))
    inverse_consistency_error_d = Iepsilon + d - approximate_Iepsilon_d
    grad_d_icon_error = (inverse_consistency_error - inverse_consistency_error_d) / delta
    direction_losses.append(torch.mean(grad_d_icon_error**2))

inverse_consistency_loss = sum(direction_losses)

```


This works great in 2d, solving the hollow triangles circles benchmark in 2 minutes instead of ~ an hour, and with more reliable and higher quality final results:

[notebook](https://colab.research.google.com/drive/1oVilftO41NREX-G7fBujQTu_QlB4U-QT?usp=sharing)

\fig{GradientICONTriangleCircles.png}


# GradientICON in 3D

Trains like a dream with not much fussing even at batch size 1, 160 x 384 x 384.

[Step 1 notebook](https://github.com/uncbiag/ICON/blob/c2732603a1e8e5e11c3bdebbb6f8949811769b53/notebooks/GradICONDice.ipynb)

Step 1 (40 x 96 x 96): DICE 66

[Step 2 notebook](https://github.com/uncbiag/ICON/blob/c2732603a1e8e5e11c3bdebbb6f8949811769b53/notebooks/GradICONDICEhires.ipynb)

Step 2 (80 x 192 x 192): DICE 71.3

[Step 3 notebook](https://github.com/uncbiag/ICON/blob/c2732603a1e8e5e11c3bdebbb6f8949811769b53/notebooks/GradICONDICEfullres.ipynb)

Step 3 (160 x 384 x 384): DICE 73.3
