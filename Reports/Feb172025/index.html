<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/css/franklin.css"> <link rel=stylesheet  href="/css/tufte.css"> <link rel=stylesheet  href="/css/latex.css"> <link rel=stylesheet  href="/css/adjust.css"> <link rel=stylesheet  href="https://apj.hgreer.com/blog"> <link rel=icon  href="/assets/favicon.png"> <title>Miserable quest for an affine universal model</title> <div id=layout > <div id=menu > <ul> <li><a href="/">Home</a> <!-- <li><a href="/FeatureMapICON/">Feature Map Inverse Consistency</a> --> <li><a href="/HatTile/">Aperiodic Tiling with Z3</a> <!-- <li><a href="/ICON/">Inverse Consistent Regstration</a> <li><a href="/Mandelbrot/">Mandelbrot Set Adventures</a> --> <li><a href="/JavascriptMandelbrot/">Mandelbrot Set</a> <li><a href="/TrebuchetSimulator/">Trebuchet Simulator</a> <li><a href="/BadMatrixMultiply/">Bad Matrix Multiplication</a> <li><a href="/GameJam/">Game Jam Games</a> <li><a href="/menu3/">Tags</a> </ul> </div> <div id=main > <div class=franklin-content ><h1>The Plan:</h1> <p>Two goals for new unigradicon: affine registration and multimodal registration</p> <p>Neural Affine registration approaches:</p> <p>Extract affine component from unigradicon output</p> <ul> <li><p>Appears to be impossible for complicated reasons &#40;willing to elaborate&#41;</p> </ul> <p>Large ConstrICON model</p> <ul> <li><p>Trains fine, works fine, training on new datasets nicely documented</p> <li><p>Large but non global capture radius</p> <li><p>Currently in use with Basar for his KL evaluation on OAI project</p> <li><p>Doesn&#39;t generalize to images not in the uniGradICON dataset - weird</p> </ul> <p>Directly optimize affine map against LNCC</p> <ul> <li><p>Works fine, doesn&#39;t require training, generalizes</p> <li><p>Considered for Basar&#39;s project but slower than previous approach</p> <li><p>Not interesting or publishable, but a good baseline</p> </ul> <p>Directly optimize affine map prior to unigradicon</p> <ul> <li><p>Works fine, doesn&#39;t require new training, generalizes</p> <li><p>Complicated, not interested in explaining again because...</p> <li><p>Worse than previous approach &#40;RIP&#41;</p> </ul> <p>Extract affine transform from equivariant registration &#40;CARL&#41;</p> <ul> <li><p>Trains ...ok, not nicely documented yet</p> <li><p>Global capture radius, which is why we use CARL on the biobank dataset- this is a major capabilities boost</p> <li><p>uniCARL Doesn&#39;t generalize to images not in the uniGradICON dataset - weird</p> </ul> <p>Use KeyMorph architecture</p> <ul> <li><p>fundamentally can&#39;t generalize to images not in the training set.</p> <li><p>global capture radius</p> <li><p>included because it emphasizes the pattern</p> </ul> <h1>Investigative steps:</h1> <p>MNIST generalization task: train on 2, 5, 8, eval on 6, 1</p> <ul> <li><p>GradICON does ok</p> <li><p>ConstrICON affine <em>passes</em> ?? &lt;- main subject for followup</p> <li><p>translation equivariant CARL passes- doesn not provide affine-ness guarantees</p> <li><p>translation equivariant CARL passes without multistep - relevant for my notes</p> <li><p>rotation equivariant CARL FAILS. &#40;addendum- passes when trained for longer &#40;&#33;&#33;?&#33;?&#33;?&#41;&#41;</p> </ul> <p><a href="/assets/Reports/Feb172025/code/ConstrICON_generalization_Test.html">ConstrICON</a> <a href="/assets/Reports/Feb172025/code/GradICON_generalization_Test.html">GradICON</a> <a href="/assets/Reports/Feb172025/code/CARL_generalization_Test.html">CARL</a> <a href="/assets/Reports/Feb172025/code/CARLrot_generalization_Test.html">CARLrot</a></p> <h1>New Theory: problem is global/local, not affine/deformable</h1> <p>Since failure of CARL ROT to generalize is replicable in small scale, it could be easy to investigate, but is unlikely to be a bug in the 3-D implementation. Expensive but useful experiment would be to train a translation equivariant univeral CARL and see if that generalizes. This model might be useful for tasks that unigradicon fails due to large translation, like the biobank scans- not actually useless.</p> <p>Since failure of ConstrICON to generalize is not replicable, it is more likely that the ConstrICON 3-D model is being held back by some bug.</p> <p>Investigation with Basar: The ConstrICON checkpoint which I picked for generalization testing last fall is bad. Rolling back to an earlier checkpoint shows promise- &#40;ConstrICON training was not previously known to be unstable&#41;</p> <h1>UK BioBank data extraction</h1> <p>Teng fei emailed several minutes ago &#40;wednesday&#41;. He got the features computed himself, needs me to register them. Will do this first priority after meeting.</p> <p>First step: Re-test generalization of uniConstrICON model</p> <h1>Submission Plan:</h1> <p>last conference of the year is NeurIPS: may 1st</p> <p>Thesis writing is in flight on <a href=" https://github.com/HastingsGreer/Thesis/blob/main/main2.tex">github</a> </p> <p>&#40;missed miccai, midl, ICML&#41;</p> <h1>side project: The torch unigradicon performance question</h1> <p>Investigation 1: biag-w05, 10 io iterations of unigradicon</p> <p>torch 1.19: 18.6 seconds</p> <p>torch 2.6: 17.5 seconds</p> <p>Investigation 2: biag-gpu6, 10 steps of gradicon training on noise</p> <p>torch 1.19: </p> <a href=/Reports/>Back to Reports</a> <div class=page-foot > <div> <a href=https://subdavis.com>subdavis.com </a> <a href=http://forrestli.con>forrestli.com</a> <div class=copyright > &copy; Hastings Greer. Last modified: February 24, 2025. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>. </div> </div> </div> </div> </div>