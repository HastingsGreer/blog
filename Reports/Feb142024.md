@def title=""


It appears that instead of regularizing the transformer with diffusion and the rest with gradICON, it's better to regularize everything with diffusion for the first few steps and everything with gradICON thereafter. This means a slightly more complex training procedure but nothing too bad.



Comparison datasets:

Lung: no change since last meeting.

Abdomen: Some images in the training dataset are upsidedown, which makes registering them harder. I need to either make my network fully equivariant to rotation or fix the dataset. The diffusion regularized approach made full equivariance a hopeless dream, but this is now maybe feasible. However I should not be lazy and fix the dataset.


IXI brain: I have keymorph running using downloaded weights. I need to replicate their synthseg labels to fully replicate their paper, after which I can run EasyReg, ANTs, and our approach on this dataset. I plan to also use KeyMorph's DLIR baseline if keymorph's registration performance replicates.

I have our approach training on the IXI dataset with the KeyMorph preprocessing.
