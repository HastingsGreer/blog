<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/css/franklin.css"> <link rel=stylesheet  href="/css/tufte.css"> <link rel=stylesheet  href="/css/latex.css"> <link rel=stylesheet  href="/css/adjust.css"> <link rel=stylesheet  href="https://apj.hgreer.com/blog"> <link rel=icon  href="/assets/favicon.png"> <title>TODO</title> <div id=layout > <div id=menu > <ul> <li><a href="/">Home</a> <!-- <li><a href="/FeatureMapICON/">Feature Map Inverse Consistency</a> --> <li><a href="/HatTile/">Aperiodic Tiling with Z3</a> <!-- <li><a href="/ICON/">Inverse Consistent Regstration</a> <li><a href="/Mandelbrot/">Mandelbrot Set Adventures</a> --> <li><a href="/JavascriptMandelbrot/">Mandelbrot Set</a> <li><a href="/TrebuchetSimulator/">Trebuchet Simulator</a> <li><a href="/BadMatrixMultiply/">Bad Matrix Multiplication</a> <li><a href="/GameJam/">Game Jam Games</a> <li><a href="/menu3/">Tags</a> </ul> </div> <div id=main > <div class=franklin-content ><p>#DONE ✓snap to grid ✓ put all results into the overleaf ✓ evaluate latest highres model ✓ train oai knees with LNCC &#43; augmentation ✓ merge biag .vimrc ✓ train brain lncc ✓ ICON 1.0 release ✓ get OAI pull request merged ✓ merge code in &quot;/media/data/hastings/InverseConsistency/&quot; into master ✓ footsteps needs to log uncommitted file if that&#39;s what&#39;s being run</p> <h1 id=todo ><a href="#todo" class=header-anchor >TODO</a></h1> <h2 id=cleanup_todo ><a href="#cleanup_todo" class=header-anchor >Cleanup todo</a></h2> <p>update <code>OAI_analysis_2</code> to ICON 1.0.0 </p> <p>log sample registrations to tensorboard</p> <p>log DICE metric to tensorboard</p> <p>fix batchfunction callback- needs refactor</p> <p>compress OAI models by dropping identitymaps</p> <p>add identitymap stripping code to ICON</p> <p>put augmentation into train.py</p> <p>ICON default network</p> <p>use better torch parallelism abastraction</p> <p>investigate corner alignment &#40;RIP LOL&#41;</p> <p>freeze network into asymmetricnet with fixed vectorfields</p> <h2 id=ambitious_todo ><a href="#ambitious_todo" class=header-anchor >Ambitious todo</a></h2> <p>train a diffusion model</p> <p>train a VQ-GAN</p> <p>genius idea: registration that improves previous result: train by randomy resetting prev result</p> <p>try segmenting latest kaggle challenge</p> <p>try registering latest kaggle challenge</p> <p>99&#37;ICON: ICON but the worst 1&#37; of inverse consistencies aren&#39;t penalized</p> <p>mixture of gaussians ICON: interpret ICON as &quot;registration outputs a gaussian at each pixel, ICON loss is log odds of exact inverse consistency. in that framework, rewrite to output a mixture of gaussians instead of a gaussian: perhaps can accurately model tearing?</p> <p>need a synthetic dataset for tearing.</p> <p>one approach to mitigate this is to create a very diverse zoo of intensity transfer functions for data augmentation, and hope that this forces the network to learn to register a larger complete set of transfer functions including MR -&gt; CT even though that&#39;s not a strict 1-1 function <a href="https://fairlydeep.slack.com/files/UKV1W0FDX/F03L71Z79PB/synthmorph_learning_contrast-invariant_registration_without_acquired_images.pdf">Adrien&#39;s paper</a></p> <h2 id=marc_request_todo ><a href="#marc_request_todo" class=header-anchor >Marc request todo</a></h2> <p>folds of lncc</p> <p>Documentation of preprocessing- more comments</p> <p>Documentation of train_batchfunction&#33;</p> <p>evaluation during training</p> <p>example of how to view warped images</p> <p>description of input_shape: the network will throw an error if the input images aren&#39;t this size.</p> <p>download full copdgene image dataset</p> <ul> <li><p>currently training with just LNCC for taste. Will try next with LNCC &#43; augmentation</p> </ul> <p>train lung with lin loss &#43; augmnentation</p> <ul> <li><p>put lin loss into losses.py- doesn&#39;t need to be embedded into GradientICON</p> </ul> <p>make formal list of experiments for paper – journal article?</p> <p>put OAI preprocessing into pretrained models</p> <p>Create a set of images where the shapes are bright and the background dark. Create another set where it is the other way around. Train a network that gets as inputs either a random image pair from one or from the other. Will this network entirely fail if it is presented with a pair where one image is from one set and the other one from the other?</p> <p>non neural finetune</p> <p>high res finetune lowres model</p> <p>contour based image visualization</p> <h2 id=itk_deficiencies_todo ><a href="#itk_deficiencies_todo" class=header-anchor >ITK deficiencies todo</a></h2> <p>verify whether itk can be installed on an old version yet?</p> <p>get itk to not segfault when asked to save a composite transform without &#91;&#93;ifying it</p> <p>get itkDisplacementFieldJacobianFilter wrapped for doubles</p> <h1 id=icon_lung_todo ><a href="#icon_lung_todo" class=header-anchor >ICON lung todo</a></h1> <p>1 dataloader 1 augmentation 1 lambda</p> <h1 id=gradicon_knee_to_do ><a href="#gradicon_knee_to_do" class=header-anchor >GradICON knee to do</a></h1> <p>Voxelmorph network with Gradient Inverse Consistency loss our lung architecture with Gradient Inverse Consistency loss</p> <a href=/Reports/>Back to Reports</a> <div class=page-foot > <div> <a href=https://subdavis.com>subdavis.com </a> <a href=http://forrestli.con>forrestli.com</a> <div class=copyright > &copy; Hastings Greer. Last modified: January 10, 2024. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>. </div> </div> </div> </div> </div>