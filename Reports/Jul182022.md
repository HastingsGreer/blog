# Atlas registration work

Making an atlas using a dataset and a registration network trained using MSE turns out to be relatively straightforward.

In 2D, I produced a colab [notebook](https://github.com/HastingsGreer/ICON_atlas_2/blob/master/2d_atlas.ipynb) demonstrating that this works.
\fig{2D-MSE-atlas.png}

There are several questions that need answers, which the 2D work leaves ambiguous:

Whether to optimize the atlas based on the similarity, or the all-up loss

Whether to initialize the atlas randomly, or with the mean of the dataset

Whether to use a penalty on the average displacement of each pixel

Notably, this last strategy works extra well in 2-D, both producing an atlas with close to zero mean displacement as well as regularizing the intensity of the image in the LNCC case.

The LNCC case is relavant: It is needed for cross-modality registration, which marc wants for registering pairs like u

\fig{Radiograph.png}\fig{DXA.png}

@@img-medium ![](DXA.png) @@

## Atlas registration in 3D

[experiments](localhost:6007/ICON_atlas_2/results/)

Roughly, in 3-D using MSE works well, especially in the setting MSE + mean-initialize + all_loss

\fig{MeanInit_MSE.png}

However, using LNCC, the results are distinctly cursed, even if mean inititialized. Notably, this happens without crazy intensity drift.

\fig{cursed_colorbar.png}

Look at this whacky patella:

\fig{whacky_patella.png}

## Atlas performance in 3D

Using the atlas generated by the mean squares technique, we can compare the performance of the ICON\_atlas algorithm using the old, pregenerated atlas to the performance of that same algorithm using the new, icon generated atlas in terms of DICE.

We get the results:

No atlas: (register directly)

71.3

Old atlas: (currently in use in oai\_analysis\_2)

DICE 70.6 

New atlas:

DICE 71.6

[notebook](https://github.com/HastingsGreer/ICON_atlas_2/blob/master/oai_dice.ipynb)

## Regularizing LNCC in 2D

In 2-D, the atlas generated by (rand\_init, LNCC, all\_loss) looks like this:

\fig{2D_lncc_unregularized}

The atlas generated by (rand\_init, LNCC, all\_loss + 40 * squared mean pixel disp) looks like this:

\fig{2D-barely-regularized.png}
_
The atlas generated by (mean\_init, LNCC, all\_loss + 900 * squared mean pixel disp) looks like this:

\fig{mean_lncc_900.png}

The atlas generated by (mean\_init, LNCC, all_loss):

\fig{mean_lnnc_unreg.png}

I'm not sure what to make of this: making an atlas out of LNCC seems hard, and the cursed wobbles seem to be a general principle.

side question: is penalizing the mean jacobian more powerful? 

finally, we can actually regularize LNCC by (mean\_init, LNCC, all\_loss + 900 * sq pix disp, extra long training) as demonstrated in this notebook.

[notebook for live investigating](https://colab.research.google.com/drive/1Jef6E2D65H5xsX6tJvhJNCzDKaF-Kl35?usp=sharing)

\fig{success_lncc.png}

# Bringing Mean displacement penalty to 3D:

Marc's math:


$$ L({u_i}) = \|\sum_{i=1}^N u_i\|_2^2 = <\sum_{i=1}^N u_i, \sum_{i=1}^N u_i > $$

Then the variation with respect to u_i is:

$$ \delta L({u_i};\delta u_i ) = 2 <\sum_{i=1}^N u_i, \delta u_i > $$

And therefore the gradient is:

$$ \nabla_{u_i} L({u_i}) = \sum_{i=1}^N u_i $$

discuss


# Lung Experiments

[tensorboard](localhost:6008)






# DICE of fine tuning highest resolution network for 50 steps on each OAI test pair:

74.91
