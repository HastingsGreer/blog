@def title="EquivariantTransformer"


$A, B$ diffeomorphisms

Want to compute $A^{-1}\circ B$.

$ = A^{-1}(B(x))$

Key A(y)

Value is y

Query is B(x)

Put all though bunch of sin waves

# Transformers naturally implement function inversion

The Key, Query, Value attention mechanism naturally lends itself to implementing function inversion in a matter appropriate for representing $\Xi$. As a first demo, we will invert the function $y=x^2$


```
x = torch.arange(0, 1, 1/100)
x = x[None, None, :]
y = x**2
```

First, we process our function inputs and outputs into feature vectors. These representations are chosen so that $repr(u) \cdot repr(v) \simeq 1$ when $u \simeq v$. 

```
scale_weight = (torch.randn(100) * 59)[:, None, None]

scale = torch.nn.Conv1d(1, 100, 1, bias=True)

with torch.no_grad():
    scale.weight[:] = scale_weight
ft_x = torch.sin(scale(x))
ft_y = torch.sin(scale(y))
```

```
plt.imshow(ft_x[0])
plt.imshow(ft_y[0])
```

\fig{ft_x.png} \fig{ft_y.png}

Then, do an attention, with function outputs as Keys, function inputs as Values, and the values that we want to pass to the inverted function as Queries.

```
attention = torch.nn.functional.softmax((ft_x.permute(0, 2, 1) @ ft_y), dim=2)
                                       
plt.imshow(attention.detach()[0])
```
\fig{attention.png}
```
output = attention @ x.permute(0, 2, 1)
plt.plot(output[0].detach())
```
\fig{output.png}






