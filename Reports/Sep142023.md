@def title="EquivariantTransformer"


In our toy problem, the images we want to register, $A, B$, are  diffeomorphisms.

To register them, we want to find $\Xi$ such that $A \circ \Xi = B$.

So, $\Xi = A^{-1} \circ B$.

A neural network that can compute $A^{-1}\circ B$ will have all the equivariances of $\Xi$.


# Transformers naturally implement function inversion

The Key, Query, Value attention mechanism naturally lends itself to implementing function inversion in a matter appropriate for representing $\Xi$. As a first demo, we will invert the function $y=x^2$


```
x = torch.arange(0, 1, 1/100)
x = x[None, None, :]
y = x**2
```

First, we process our function inputs and outputs into feature vectors. These representations are chosen so that $repr(u) \cdot repr(v) \simeq 1$ when $u \simeq v$. 

```
scale_weight = (torch.randn(100) * 59)[:, None, None]

scale = torch.nn.Conv1d(1, 100, 1, bias=True)

with torch.no_grad():
    scale.weight[:] = scale_weight
ft_x = torch.sin(scale(x))
ft_y = torch.sin(scale(y))
```

```
plt.imshow(ft_x[0])
plt.imshow(ft_y[0])
```

\fig{ft_x.png} \fig{ft_y.png}

Then, do an attention, with function outputs as Keys, function inputs as Values, and the values that we want to pass to the inverted function as Queries.

```
attention = torch.nn.functional.softmax((ft_x.permute(0, 2, 1) @ ft_y), dim=2)
                                       
plt.imshow(attention.detach()[0])
```
\fig{attention.png}
```
output = attention @ x.permute(0, 2, 1)
plt.plot(output[0].detach())
```
\fig{output.png}


Voila, the graph of $\sqrt{x}$.

# Early Concern about boundary conditions

If we invert sin(x) on the same interval (0, 1) we see some ugliness because the domain and range don't match.

\fig{arcsin.png}



