<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/libs/highlight/github.min.css"> <link rel=stylesheet  href="/css/franklin.css"> <link rel=stylesheet  href="/css/tufte.css"> <link rel=stylesheet  href="/css/latex.css"> <link rel=stylesheet  href="/css/adjust.css"> <link rel=stylesheet  href="https://apj.hgreer.com/blog"> <link rel=icon  href="/assets/favicon.png"> <title>continued retina experiments</title> <div id=layout > <div id=menu > <ul> <li><a href="/">Home</a> <!-- <li><a href="/FeatureMapICON/">Feature Map Inverse Consistency</a> --> <li><a href="/HatTile/">Aperiodic Tiling with Z3</a> <!-- <li><a href="/ICON/">Inverse Consistent Regstration</a> <li><a href="/Mandelbrot/">Mandelbrot Set Adventures</a> --> <li><a href="/JavascriptMandelbrot/">Mandelbrot Set</a> <li><a href="/TrebuchetSimulator/">Trebuchet Simulator</a> <li><a href="/BadMatrixMultiply/">Bad Matrix Multiplication</a> <li><a href="/GameJam/">Game Jam Games</a> <li><a href="/menu3/">Tags</a> </ul> </div> <div id=main > <div class=franklin-content ><p>The first question we tackled was whether there was some code difference between the 3-d and 2-d registration models. We reduced the chances of this by refactoring the code base so that the 2-d and 3-d codes use the same codebase, with a minimum of if-statements guarding dimensionality specific code. This is easier to verify than the previous project of comparing jupyter notebooks for 2-D code and python scripts for 3-D registration. Unfortunately, it has not revealed any discrepancy- the previous notebook-script mess was correct.</p> <p>The second question is what happens in a 3-D triangles and circles situation. We investigated this by training just registering the lung masks, which is a much simpler task. The network was able to align them, and the transformer and displacement predictors shared work like &#91;&#93;</p> <p>The third question is &quot;is the retina registration transformer only able to capture complex transforms because it memorizes the train set?&quot; We evaluate it on a held out test set and it captures the detailed transform, making this unlikely.</p> <p>Huge breakthrough from lin: turning up the diffusion regularization in the split regularizer replicates the problem in 2-d. Turning down the diffusion regularization has no effect in 2d, other than slightly slower initialization &#40;stays scrambled eggs longer&#41;. Trying now in 3d </p> <p>&#40;figure of 2d result with over regularization&#41;</p> <p>whole network</p> <img src="/assets/Reports/Jan242024/code/over_reg_whole.png" alt=""> <p>just transformer</p> <img src="/assets/Reports/Jan242024/code/over_reg_just_transformer.png" alt=""> <p>&#40;figure of 2d result with under-regularization&#41;</p> <img src="/assets/Reports/Jan242024/code/020_after_registration.png" alt=""> <p>In 3d, turning down the diffusion regularization causes a moderate improvement in similarity, and more importantly successfully caused the transformer to begin doing its share of the work.</p> <p>MTre results:</p> <p>With diffusion_reg&#61;0.02, </p> <pre><code class="julia hljs">Full network error on copdgene1: <span class=hljs-number >5.1</span>
Just equivariant transformer: <span class=hljs-number >5.5</span></code></pre> <p>With diffusion_reg&#61;1.5 &#40;old setting&#41;</p> <pre><code class="julia hljs">Full network on copdgene1: <span class=hljs-number >6.6</span>
Just equivariant transformer: <span class=hljs-number >12</span></code></pre> <p>This is just with overnight training, we will see if training more or other hacks improves results further.</p> <p>results at </p> <pre><code class="julia hljs">/playpen-raid1/tgreer/equivariant_reg_2/evaluation_results/reg_<span class=hljs-number >.15</span>-<span class=hljs-number >1</span>/,
/playpen-raid1/tgreer/equivariant_reg_2/evaluation_results/reg_<span class=hljs-number >.15_</span>just_equivar/</code></pre> <p>3d just transformer</p> <img src="/assets/Reports/Jan242024/code/3d-just-transformer.png" alt=""> <p>The improvement in mtre is modest &#40;1.5 mm in in this setting with matched, abbreviated training&#41; which is probably why this issue was not noticed earlier, but this is major progress in internal network behavior.</p> <a href=/Reports/>Back to Reports</a> <div class=page-foot > <div> <a href=https://subdavis.com>subdavis.com </a> <a href=http://forrestli.con>forrestli.com</a> <div class=copyright > &copy; Hastings Greer. Last modified: March 15, 2024. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>. </div> </div> </div> </div> </div>