@def title="By Construction ICON, progressive training vs ent to end"


# By Construction ICON

if we build an network that is inverse consistent by construction, then penalize it with the ICON error, how regularized is it? Does it become regularized by adding in noise?

\begin{align}
 v(x) &= N_\theta^{AB}(x) - N_\theta^{BA}(x) \\
 \frac{\partial}{\partial t} \Phi^{AB}(x, t) &= v(x)\\
 \Phi^{AB}(x, 0) &= x \\
 \Phi^{AB}(x) &= \Phi^{AB}(x, 1)
\end{align}


[notebook for byconstructionICON](https://colab.research.google.com/drive/1fPt6R3ZkbMJ_ntQHUkbQElhjQm9Z7Wax?usp=sharing)

results depend heavily on loss

Blurred SSD ConstICON

\fig{blurredssd.png}

Blurred SSD NoReg

\fig{blurredssdnoreg.png}

SSD ConstICON

\fig{ssdconsticon.png.png}

SSD\_only\_interpolated\_consticon

\fig{ssd_only_interpolated_consticon.png}


# Simplified progressive train pipeline

Train the classic 

```
inner_net = icon.FunctionFromVectorField(networks.tallUNet2(dimension=3))

    for _ in range(2):
         inner_net = icon.TwoStepRegistration(
             icon.DownsampleRegistration(inner_net, dimension=3),
             icon.FunctionFromVectorField(networks.tallUNet2(dimension=3))
         )
```

network end to end, then tack on an extra step at the final resolution.

[Preprocessing data](https://github.com/uncbiag/ICON/blob/brain_evaluation/training_scripts/gradICON/preprocess_train_halfres_knees.py)

[End to end train most of it](https://github.com/uncbiag/ICON/blob/brain_evaluation/training_scripts/gradICON/gradicon_knee_halfres_new.py)

[Tack on one more layer](https://github.com/uncbiag/ICON/blob/brain_evaluation/training_scripts/gradICON/gradicon_knee_halfres_new_2ndhalfres.py)

\fig{loss_knee_2.png}

Performance:

```
DICE: tensor(0.7089)
mean folds: 0.17333333333333334
```

Calculation [notebook](https://github.com/uncbiag/ICON/blob/brain_evaluation/notebooks/GradICON_add_second_halfres_DICE.ipynb)

