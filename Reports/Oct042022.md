@def title="By Construction ICON, progressive training vs ent to end"


# By Construction ICON

if we build an network that is inverse consistent by construction, then penalize it with the ICON error, how regularized is it? Does it become regularized by adding in noise?

\begin{align}
 v(x) &= N_\theta^{AB}(x) - N_\theta^{BA}(x) \\
 \frac{\partial}{\partial t} \Phi^{AB}(x_0, t) &= v(\Phi^{AB}(x_0, t))\\
 \Phi^{AB}(x, 0) &= x \\
 \Phi^{AB}(x) &= \Phi^{AB}(x, 1)
\end{align}


$$ $$


[notebook for byconstructionICON](https://colab.research.google.com/drive/1fPt6R3ZkbMJ_ntQHUkbQElhjQm9Z7Wax?usp=sharing)

results depend heavily on loss

Blurred SSD ConstICON

\fig{blurredssd.png}

Blurred SSD NoReg

\fig{blurredssdnoreg.png}

SSD ConstICON

\fig{ssdconsticon.png.png}

SSD\_only\_interpolated\_consticon

\fig{ssd_only_interpolated_consticon.png}


# Simplified progressive train pipeline

Train the classic 

```
inner_net = icon.FunctionFromVectorField(networks.tallUNet2(dimension=3))

for _ in range(2):
    inner_net = icon.TwoStepRegistration(
        icon.DownsampleRegistration(inner_net, dimension=3),
        icon.FunctionFromVectorField(networks.tallUNet2(dimension=3))
    )
```

network end to end, then tack on an extra step at the final resolution.

[Preprocessing data](https://github.com/uncbiag/ICON/blob/brain_evaluation/training_scripts/gradICON/preprocess_train_halfres_knees.py)

[End to end train most of it](https://github.com/uncbiag/ICON/blob/brain_evaluation/training_scripts/gradICON/gradicon_knee_halfres_new.py)

[Tack on one more layer](https://github.com/uncbiag/ICON/blob/brain_evaluation/training_scripts/gradICON/gradicon_knee_halfres_new_2ndhalfres.py)

```
inner_net = icon.FunctionFromVectorField(networks.tallUNet2(dimension=3))

for _ in range(2):
    inner_net = icon.twostepregistration(
        icon.downsampleregistration(inner_net, dimension=3),
        icon.functionfromvectorfield(networks.tallunet2(dimension=3))
    )
inner_net = icon.twostepregistration(
    inner_net, icon.functionfromvectorfield(networks.tallunet2(dimension=3))
)
```

this second step is trained using only half res gradicon but the same accurate finite differences

\fig{loss_knee_2.png}

performance:

```
dice: tensor(0.7089)
mean folds: 0.17333333333333334
```

calculation [notebook](https://github.com/uncbiag/icon/blob/brain_evaluation/notebooks/gradicon_add_second_halfres_dice.ipynb)


# Paper work:

Convergence argument: I don't think that the o


Tag on: for both end to end and progressive, tag on fine tuning

Add a few sentences explaining that large channel counts at low resolution are basically free.

immediate todo: put sparse into master


Marc's paper that he is curious about

https://www.sciencedirect.com/science/article/pii/S1361841522000858

Mattias Heinrich's paper contains ANT, elastix lung results

how does it do so good?
