@def title=""


The Plan:
=========

Two goals for new unigradicon: affine registration and multimodal registration


Neural Affine registration approaches:

Extract affine component from unigradicon output
- Appears to be impossible for complicated reasons (willing to elaborate)

Large ConstrICON model
- Trains fine, works fine, training on new datasets nicely documented
- Large but non global capture radius
- Currently in use with Basar for his KL evaluation on OAI project
- Doesn't generalize to images not in the uniGradICON dataset - weird

Directly optimize affine map against LNCC
- Works fine, doesn't require training, generalizes
- Considered for Basar's project but slower than previous approach
- Not interesting or publishable, but a good baseline

Directly optimize affine map prior to unigradicon
- Works fine, doesn't require new training, generalizes
- Complicated, not interested in explaining again because...
- Worse than previous approach (RIP)

Extract affine transform from equivariant registration (CARL)
- Trains ...ok, not nicely documented yet
- Global capture radius, which is why we use CARL on the biobank dataset- this is a major capabilities boost
- uniCARL Doesn't generalize to images not in the uniGradICON dataset - weird

Use KeyMorph architecture
- fundamentally can't generalize to images not in the training set.
- global capture radius
- included because it emphasizes the pattern



Investigative steps:
====================

MNIST generalization task: train on 2, 5, 8, eval on 6, 1

- GradICON deformable passes
- ConstrICON affine _passes_ ?? <- main subject for followup
- CARL 











side project: The torch unigradicon performance question
=========================================

Investigation 1: biag-w05, 10 io iterations of unigradicon

torch 1.19: 18.6 seconds


torch 2.6: 17.5 seconds

Investigation 2: biag-gpu6, 10 steps of gradicon training on noise

torch 1.19: 

